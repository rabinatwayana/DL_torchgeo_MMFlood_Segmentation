{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabinatwayana/DL_torchgeo_MMFlood_Segmentation/blob/master/Drive_mmflood.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca706c54",
      "metadata": {
        "id": "ca706c54"
      },
      "source": [
        "# Deep Learning-based Semantic Flood Mapping using UNet and SegFormer on Multimodal Earth Observation Data\n",
        "\n",
        "Author: Rabina Twayana\n",
        "\n",
        "This notebook aims to perform semantic segmentation for flood mapping by training, evaluating, and comparing different deep learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9DaNwrSTF1Ql",
      "metadata": {
        "id": "9DaNwrSTF1Ql"
      },
      "source": [
        "## Environment and Install Packages\n",
        "\n",
        "The notebook was run in Google Colab. Following packages were installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f00ff37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f00ff37",
        "outputId": "f348195e-e0cb-407c-d756-0033a3f3cc74"
      },
      "outputs": [],
      "source": [
        "# Case: Google Colab\n",
        "# !pip install torchgeo\n",
        "# !pip install wandb\n",
        "\n",
        "# Case: Local (using conda)\n",
        "# conda create -n torchgeo_env python=3.11\n",
        "# conda activate torchgeo_env  \n",
        "# conda install -c conda-forge torchgeo\n",
        "# !conda install wandb -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07927b63",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FlIoR1j6F_sU",
      "metadata": {
        "id": "FlIoR1j6F_sU"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "YTyVBwYwGC6u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTyVBwYwGC6u",
        "outputId": "feea51cd-dfd5-4607-906a-04c9afedf289"
      },
      "outputs": [],
      "source": [
        "from lightning.pytorch import Trainer\n",
        "from lightning.pytorch.loggers import WandbLogger ##https://lightning.ai/docs/pytorch/stable/extensions/generated/lightning.pytorch.loggers.WandbLogger.html#lightning.pytorch.loggers.WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torchgeo.trainers import SemanticSegmentationTask\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "from torchgeo.datasets import MMFlood\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# # import wandb\n",
        "# # wandb.login()\n",
        "\n",
        "# # Project that the run is recorded to\n",
        "# # project = \"MMFlood_DL_Experiments\"\n",
        "\n",
        "\n",
        "# checkpoint_callback = ModelCheckpoint(\n",
        "#     monitor=\"val_iou\",        # metric to monitor (IoU in your case)\n",
        "#     mode=\"max\",               # save the checkpoint with max val_iou\n",
        "#     save_top_k=1,             # save only the best model\n",
        "#     filename=\"best-{epoch:02d}-{val_iou:.4f}\"\n",
        "# )\n",
        "\n",
        "\n",
        "# # UNet run\n",
        "# # wandb_logger_unet = WandbLogger(\n",
        "# #     project=\"MMFlood_DL_Experiments\",\n",
        "# #     name=\"unet\"\n",
        "# # )\n",
        "\n",
        "# # SegFormer run\n",
        "# wandb_logger_segformer = WandbLogger(\n",
        "#     project=\"MMFlood_DL_Experiments\",\n",
        "#     name=\"segformer\"\n",
        "# )\n",
        "\n",
        "\n",
        "# trainer = Trainer(logger=wandb_logger_segformer, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y8O_i9lAGa-4",
      "metadata": {
        "id": "Y8O_i9lAGa-4"
      },
      "source": [
        "## Dataset\n",
        "MMFlood dataset is a multimodal flood delineation dataset (Montello et al., 2022).\n",
        "\n",
        " Some Sentinel-1 tiles have missing data, which are automatically set to 0. Corresponding pixels in masks are set to 255 and should be ignored in performance computation.\n",
        "\n",
        "Dataset features:\n",
        "\n",
        "- 1,748 Sentinel-1 tiles of varying pixel dimensions\n",
        "\n",
        "- Multimodal dataset (Sentinel-1, DEMs and hydrography maps (available for 1,012 tiles out of 1,748))\n",
        "\n",
        "- 95 flood events from 42 different countries ranging from 2014 to 2021\n",
        "\n",
        "- Flood delineation maps (ground truth) is obtained from Copernicus EMS\n",
        "\n",
        "- Missing data in Sentinel-1 tiles are set to 0 and corrsponding pixels in masks are set to 255 (must ignored in performance computation)  \n",
        "\n",
        "- Image size is 2000 * 2000\n",
        "\n",
        "Dataset classes:\n",
        "\n",
        "- no flood\n",
        "\n",
        "- flood\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f019f636",
      "metadata": {},
      "source": [
        "### Download data\n",
        "\n",
        "If data is already exist in root dir, download will be skipped. Data download failed when I tried in colab. So downlaod and data subset was done locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZjwIqz2qrFOx",
      "metadata": {
        "id": "ZjwIqz2qrFOx"
      },
      "outputs": [],
      "source": [
        "dataset = MMFlood(\n",
        "    root=\"data\",   # where data will be stored\n",
        "    download=True,         # this triggers download\n",
        "    checksum=True          # optional but recommended\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f3f217",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_activations_stats(annot_path):\n",
        "    # Load JSON\n",
        "    with open(annot_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Convert dict to DataFrame\n",
        "    df = pd.DataFrame.from_dict(data, orient=\"index\")\n",
        "    print(\"Column List: \",list(df.columns))\n",
        "\n",
        "    country_counts = df[\"country\"].value_counts()\n",
        "    total_countries = country_counts.shape[0]\n",
        "\n",
        "    print(f\"\\nTotal Countries: {total_countries}\")\n",
        "\n",
        "\n",
        "    print(f\"\\nTotal activations: {len(df)}\")\n",
        "\n",
        "    subset_counts = df[\"subset\"].value_counts()\n",
        "    print(\"\\nTotal train/test/val count\\n\",subset_counts)\n",
        "\n",
        "    \n",
        "\n",
        "    table_counts = pd.crosstab(\n",
        "        df[\"country\"],\n",
        "        df[\"subset\"]\n",
        "    )\n",
        "    table_counts[\"total\"]=table_counts[\"train\"]+table_counts[\"test\"]+table_counts[\"val\"]\n",
        "    print(\"\\nCountry wise Train/Test/Val activation count \")\n",
        "    table_counts_sorted = table_counts.sort_values(\n",
        "        by=\"test\",\n",
        "        ascending=False\n",
        "    )\n",
        "    print(table_counts_sorted)\n",
        "\n",
        "    # Reset index to keep activation ID\n",
        "    # df = df.reset_index().rename(columns={\"index\": \"activation_id\"})\n",
        "\n",
        "\n",
        "get_activations_stats(\"data/original_activations.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b06a300c",
      "metadata": {},
      "source": [
        "### Removed activations from TEST dataset\n",
        "- France (EMSR492, EMSR411)\n",
        "- Italy (EMSR333,EMSR141, EMSR330,EMSR496,EMSR548)\n",
        "- Ireland (EMSR149)\n",
        "\n",
        "### Move from train to val\n",
        "- Germany (EMSR497)\n",
        "- Ireland (EMSR156)\n",
        "- Greece (EMSR117)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "156cc780",
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_countries= [\"Greece\", \"Spain\", \"France\", \"Italy\", \"Germany\", \"UK\", \"Australia\", \"Ireland\"]\n",
        "delete_test_activations = ['EMSR492','EMSR411','EMSR333','EMSR141','EMSR330','EMSR496','EMSR548','EMSR149']\n",
        "train_to_val_activations = ['EMSR497','EMSR156','EMSR117']\n",
        "\n",
        "\n",
        "def run_data_selection(original_json_path, new_json_path, selected_countries=[], delete_test_activations=[],train_to_val_activations=[] ):\n",
        "    # Paths\n",
        "    # root = Path(\"data/activations\")  # path to your dataset\n",
        "    # original_json =  \"data/activations.json\"\n",
        "    # new_json = \"data/selected_activations.json\"\n",
        "\n",
        "    # Load existing annotations\n",
        "    with open(original_json_path) as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    # Filter tiles: remove or ignore tiles with empty hydro folders\n",
        "    selected_metadata = {}\n",
        "    for tile_id, tile_info in metadata.items():\n",
        "        if tile_info[\"country\"] in selected_countries:\n",
        "            selected_metadata[tile_id]=tile_info\n",
        "\n",
        "    for tile_id in list(selected_metadata.keys()):  # make a copy of keys\n",
        "        if tile_id in delete_test_activations:\n",
        "            print(\"yes\")\n",
        "            del selected_metadata[tile_id]\n",
        "\n",
        "    # selected_metadata_2 = {}\n",
        "    # print(selected_metadata.items())\n",
        "    # for tile_id, tile_info in selected_metadata.items():\n",
        "    #     if tile_id in delete_test_activations:\n",
        "    #         print(\"yes\")\n",
        "    #         continue\n",
        "    #     selected_metadata_2[tile_id]=tile_info\n",
        "    \n",
        "    updated_metadata = {}\n",
        "    for tile_id, tile_info in selected_metadata.items():\n",
        "        if tile_id in train_to_val_activations:\n",
        "            tile_info[\"subset\"]=\"val\"\n",
        "            updated_metadata[tile_id]=tile_info\n",
        "\n",
        "    # # Save new JSON\n",
        "    with open(new_json_path, \"w\") as f:\n",
        "        json.dump(selected_metadata, f, indent=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e92717e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# MMFlood reads the activations.json file inside data root directory.\n",
        "import os\n",
        "os.rename(\"data/activations.json\", \"data/original_activations.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b2642cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "original_json_path = \"data/original_activations.json\"\n",
        "new_json_path = \"data/activations.json\"\n",
        "\n",
        "run_data_selection(original_json_path, new_json_path, selected_countries, delete_test_activations,train_to_val_activations )\n",
        "get_activations_stats(new_json_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa3418a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "def create_selected_data_folder(act_json_path, activations_dir, target_dir):\n",
        "    # Load metadata\n",
        "    with open(act_json_path) as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    df = pd.DataFrame.from_dict(metadata, orient=\"index\")\n",
        "    df = df.reset_index().rename(columns={\"index\": \"activation_id\"})\n",
        "    activation_ids = df[\"activation_id\"].astype(str).tolist()\n",
        "\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    copied = 0\n",
        "    missing = []\n",
        "\n",
        "    # List folders once (much faster)\n",
        "    all_folders = [\n",
        "        d for d in os.listdir(activations_dir)\n",
        "        if os.path.isdir(os.path.join(activations_dir, d))\n",
        "    ]\n",
        "\n",
        "    for act_id in activation_ids:\n",
        "        # Find matching folders\n",
        "        matched = [d for d in all_folders if d.startswith(act_id)]\n",
        "\n",
        "        if not matched:\n",
        "            missing.append(act_id)\n",
        "            continue\n",
        "\n",
        "        for folder in matched:\n",
        "            # print(folder)\n",
        "            src = os.path.join(activations_dir, folder)\n",
        "            dst = os.path.join(target_dir, folder)\n",
        "\n",
        "            if not os.path.exists(dst):\n",
        "                shutil.copytree(src, dst)\n",
        "                copied += 1\n",
        "\n",
        "    print(f\"Copied {copied} folders\")\n",
        "    if missing:\n",
        "        print(f\"No folder found for  activation IDs: {missing}\")\n",
        "create_selected_data_folder( \"data/activations.json\",\"data/activations\",\"selected_data/activations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f85176",
      "metadata": {},
      "outputs": [],
      "source": [
        "shutil.move(\"data/activations.json\", \"selected_data/activations.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "719b0e73",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy selected data\n",
        "import shutil\n",
        "import os\n",
        "def create_selected_data_folder(act_json_path, activations_dir, target_dir):\n",
        "    with open(act_json_path) as f:\n",
        "        metadata = json.load(f)\n",
        "        df = pd.DataFrame.from_dict(metadata, orient=\"index\")\n",
        "        df = df.reset_index().rename(columns={\"index\": \"activation_id\"})\n",
        "        activation_ids=list(df['activation_id'])\n",
        "\n",
        "        # Ensure target directory exists\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "        copied = 0\n",
        "        missing = []\n",
        "\n",
        "        for act_id in activation_ids:\n",
        "            src = os.path.join(activations_dir, act_id)\n",
        "            dst = os.path.join(target_dir, act_id)\n",
        "\n",
        "            if os.path.isdir(src):\n",
        "                if not os.path.exists(dst):\n",
        "                    # shutil.copytree(src, dst)\n",
        "                    copied += 1\n",
        "            else:\n",
        "                missing.append(act_id)\n",
        "\n",
        "        if missing:\n",
        "            print(f\"Missing activations: {missing}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d216d254",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5376123f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# train_dataset = MMFlood(root=\"data/\", split=\"train\", include_hydro=True)\n",
        "# val_dataset = MMFlood(root=\"data/\", split=\"val\", include_hydro=False)\n",
        "# test_dataset = MMFlood(root=\"data/\", split=\"test\", include_hydro=False)\n",
        "\n",
        "# print(\"Train samples:\", len(train_dataset))\n",
        "# print(\"Val samples:\", len(val_dataset))\n",
        "# print(\"Test samples:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cca01147",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XjG1AHYLP1GH",
      "metadata": {
        "id": "XjG1AHYLP1GH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X6XdBGY7IjDD",
      "metadata": {
        "id": "X6XdBGY7IjDD"
      },
      "source": [
        "## Deeplearning Models\n",
        "\n",
        "### a) Unet (Base model)\n",
        "\n",
        "### b) SegFormer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fTlx-kD4LkLv",
      "metadata": {
        "id": "fTlx-kD4LkLv"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a3897e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 551\n",
            "Val samples: 127\n",
            "Test samples: 109\n"
          ]
        }
      ],
      "source": [
        "train_dataset = MMFlood(root=\"selected_data/\", split=\"train\", include_hydro=False, include_dem=True)\n",
        "val_dataset = MMFlood(root=\"selected_data/\", split=\"val\", include_hydro=False,include_dem=True)\n",
        "test_dataset = MMFlood(root=\"selected_data/\", split=\"test\", include_hydro=False,include_dem=True)\n",
        "\n",
        "print(\"Train samples:\", len(train_dataset))\n",
        "print(\"Val samples:\", len(val_dataset))\n",
        "print(\"Test samples:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7058ae90",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "indices = list(range(len(train_dataset)))\n",
        "train_subset = Subset(train_dataset, indices)\n",
        "train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b59727e6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x1978ec690>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "577edc60",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "/Users/rabinatwayana/miniforge3/envs/torchgeo_env/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:400: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
            "\n",
            "  | Name          | Type             | Params | Mode  | FLOPs\n",
            "-------------------------------------------------------------------\n",
            "0 | model         | Segformer        | 24.8 M | train | 0    \n",
            "1 | criterion     | CrossEntropyLoss | 0      | train | 0    \n",
            "2 | train_metrics | MetricCollection | 0      | train | 0    \n",
            "3 | val_metrics   | MetricCollection | 0      | train | 0    \n",
            "4 | test_metrics  | MetricCollection | 0      | train | 0    \n",
            "-------------------------------------------------------------------\n",
            "24.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "24.8 M    Total params\n",
            "99.337    Total estimated model params size (MB)\n",
            "181       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "0         Total Flops\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rabinatwayana/miniforge3/envs/torchgeo_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:429: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
            "/Users/rabinatwayana/miniforge3/envs/torchgeo_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "                                                                           \r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rabinatwayana/miniforge3/envs/torchgeo_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:429: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Epoch 0:   9%|‚ñâ         | 117/1250 [12:03<1:56:50,  0.16it/s, v_num=y83h]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# from torch.utils.data import DataLoader\n",
        "# from pytorch_lightning import Trainer\n",
        "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "# from pytorch_lightning.loggers import WandbLogger\n",
        "# from datetime import datetime\n",
        "\n",
        "# # TorchGeo imports\n",
        "from torchgeo.samplers import RandomGeoSampler\n",
        "from torchgeo.datasets import stack_samples\n",
        "# from torchgeo.trainers import SemanticSegmentationTask\n",
        "\n",
        "def train_model(\n",
        "    model_name: str,\n",
        "    input_type: str,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    max_epochs: int = 50,\n",
        "    batch_size: int = 8,\n",
        "    patch_size: int = 256,          # size of random patches (in pixels)\n",
        "    num_train_patches: int = 10000, # how many random patches per epoch\n",
        "    num_val_patches: int = 2000,\n",
        "    in_channels: int = 3,           # IMPORTANT: set this to your actual number of input bands!\n",
        "    num_classes: int = 2,\n",
        "    learning_rate: float = 0.001,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a TorchGeo semantic segmentation model (UNet or SegFormer) with proper geospatial sampling.\n",
        "    \n",
        "    Args:\n",
        "        model_name (str): 'unet' or 'segformer'\n",
        "        input_type (str): description of input bands (used for logging/checkpoint naming)\n",
        "        train_dataset, val_dataset: TorchGeo geospatial datasets (e.g. MMFlood or IntersectionDataset)\n",
        "        max_epochs (int): number of training epochs\n",
        "        batch_size (int): batch size\n",
        "        patch_size (int): size of random patches (square)\n",
        "        num_train_patches (int): number of random patches per epoch for training\n",
        "        num_val_patches (int): number of random patches per epoch for validation\n",
        "        in_channels (int): number of input channels (bands) ‚Äî MUST match your data!\n",
        "        num_classes (int): number of output classes (including background if needed)\n",
        "        learning_rate (float): initial learning rate\n",
        "\n",
        "    Returns:\n",
        "        str: path to the best saved checkpoint\n",
        "    \"\"\"\n",
        "    # 1. Create proper TorchGeo samplers (this fixes the TypeError!)\n",
        "    train_sampler = RandomGeoSampler(\n",
        "        train_dataset,\n",
        "        size=patch_size,\n",
        "        length=num_train_patches,\n",
        "        # res=...  # optional: if you want to force a specific resolution\n",
        "    )\n",
        "\n",
        "    val_sampler = RandomGeoSampler(\n",
        "        val_dataset,\n",
        "        size=patch_size,\n",
        "        length=num_val_patches,\n",
        "    )\n",
        "\n",
        "    # 2. Create DataLoaders with the samplers\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=train_sampler,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        collate_fn=stack_samples,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=val_sampler,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        collate_fn=stack_samples,\n",
        "    )\n",
        "\n",
        "    # 3. Dynamic Weights & Biases logger\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    wandb_logger = WandbLogger(\n",
        "        project=\"MMFlood_DL_Experiments\",\n",
        "        name=f\"{model_name}_{input_type}_{timestamp}\",\n",
        "        log_model=\"all\",  # optional: log best model\n",
        "    )\n",
        "\n",
        "    # 4. Checkpoint callback (saves best model based on val IoU)\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor=\"val_iou\",\n",
        "        mode=\"max\",\n",
        "        save_top_k=1,\n",
        "        filename=f\"best-{model_name}-{{epoch:02d}}-{{val_iou:.4f}}\",\n",
        "        dirpath=f\"checkpoints/{model_name}_{input_type}\",\n",
        "        auto_insert_metric_name=False,\n",
        "    )\n",
        "\n",
        "    # 5. Create the LightningModule (SemanticSegmentationTask)\n",
        "    task = SemanticSegmentationTask(\n",
        "        model=model_name,           # \"unet\" or \"segformer\"\n",
        "        backbone=\"resnet50\",        # used for UNet; ignored or optional for SegFormer\n",
        "        in_channels=in_channels,    # ‚Üê CRITICAL: must match your actual input bands!\n",
        "        num_classes=num_classes,\n",
        "        loss=\"ce\",                  # cross-entropy\n",
        "        ignore_index=255,           # usually used for invalid/no-data pixels\n",
        "        lr=learning_rate,\n",
        "        # class_weights=...,        # optional\n",
        "        # weights=\"imagenet\",       # optional: for pretrained backbones\n",
        "    )\n",
        "\n",
        "    # 6. Initialize PyTorch Lightning Trainer\n",
        "    trainer = Trainer(\n",
        "        max_epochs=max_epochs,\n",
        "        accelerator=\"auto\",         # \"gpu\", \"cpu\", \"mps\"...\n",
        "        devices=1,\n",
        "        logger=wandb_logger,\n",
        "        callbacks=[checkpoint_callback],\n",
        "        # precision=\"16-mixed\",     # optional: mixed precision training\n",
        "        # log_every_n_steps=10,\n",
        "        # check_val_every_n_epoch=5,\n",
        "    )\n",
        "\n",
        "    # 7. Train the model\n",
        "    trainer.fit(\n",
        "        model=task,\n",
        "        train_dataloaders=train_loader,\n",
        "        val_dataloaders=val_loader,\n",
        "    )\n",
        "\n",
        "    # 8. Return path to the best checkpoint\n",
        "    return checkpoint_callback.best_model_path\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Example usage\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "    # Assume these are already created as TorchGeo datasets\n",
        "    # train_dataset = ...\n",
        "    # val_dataset = ...\n",
        "\n",
        "    # # Example 1: Train U-Net\n",
        "    # best_unet_ckpt = train_model(\n",
        "    #     model_name=\"unet\",\n",
        "    #     input_type=\"s1_dem_hydro\",\n",
        "    #     train_dataset=train_dataset,\n",
        "    #     val_dataset=val_dataset,\n",
        "    #     max_epochs=50,\n",
        "    #     batch_size=8,\n",
        "    #     patch_size=256,\n",
        "    #     in_channels=5,          # ‚Üê CHANGE THIS to match your actual number of bands!\n",
        "    #     num_classes=2,\n",
        "    # )\n",
        "    # print(\"Best UNet checkpoint:\", best_unet_ckpt)\n",
        "\n",
        "# Example 2: Train SegFormer\n",
        "best_segformer_ckpt = train_model(\n",
        "    model_name=\"segformer\",\n",
        "    input_type=\"s1_dem_hydro\",\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    max_epochs=50,\n",
        "    batch_size=8,\n",
        "    patch_size=256,\n",
        "    in_channels=2,          # ‚Üê CHANGE THIS to match your actual number of bands!\n",
        "    num_classes=2,\n",
        ")\n",
        "print(\"Best SegFormer checkpoint:\", best_segformer_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bh5MDimPH4qU",
      "metadata": {
        "id": "bh5MDimPH4qU"
      },
      "source": [
        "## References\n",
        "Montello, F., Arnaudo, E., & Rossi, C. (2022). MMFlood: A Multimodal Dataset for Flood Delineation From Satellite Imagery. IEEE Access, 10, 96774‚Äì96787. https://doi.org/10.1109/ACCESS.2022.3205419\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torchgeo_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
