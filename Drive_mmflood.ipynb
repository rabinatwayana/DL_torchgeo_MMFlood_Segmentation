{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabinatwayana/DL_torchgeo_MMFlood_Segmentation/blob/master/Drive_mmflood.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca706c54",
      "metadata": {
        "id": "ca706c54"
      },
      "source": [
        "# Deep Learning-based Semantic Flood Mapping using UNet and SegFormer on Multimodal Earth Observation Data\n",
        "\n",
        "Author: Rabina Twayana\n",
        "\n",
        "This notebook aims to perform semantic segmentation for flood mapping by training, evaluating, and comparing different deep learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Packages\n",
        "\n",
        "The notebook was run in Google Colab. Following packages were installed."
      ],
      "metadata": {
        "id": "9DaNwrSTF1Ql"
      },
      "id": "9DaNwrSTF1Ql"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f00ff37",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "6f00ff37"
      },
      "outputs": [],
      "source": [
        "!pip install torchgeo\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages"
      ],
      "metadata": {
        "id": "FlIoR1j6F_sU"
      },
      "id": "FlIoR1j6F_sU"
    },
    {
      "cell_type": "code",
      "source": [
        "import torchgeo\n",
        "from lightning.pytorch import Trainer\n",
        "from lightning.pytorch.loggers import WandbLogger ##https://lightning.ai/docs/pytorch/stable/extensions/generated/lightning.pytorch.loggers.WandbLogger.html#lightning.pytorch.loggers.WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torchgeo.trainers import SemanticSegmentationTask\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# import wandb\n",
        "# wandb.login()\n",
        "\n",
        "# Project that the run is recorded to\n",
        "# project = \"MMFlood_DL_Experiments\"\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor=\"val_iou\",        # metric to monitor (IoU in your case)\n",
        "    mode=\"max\",               # save the checkpoint with max val_iou\n",
        "    save_top_k=1,             # save only the best model\n",
        "    filename=\"best-{epoch:02d}-{val_iou:.4f}\"\n",
        ")\n",
        "\n",
        "\n",
        "# UNet run\n",
        "wandb_logger_unet = WandbLogger(\n",
        "    project=\"MMFlood_DL_Experiments\",\n",
        "    name=\"unet\"\n",
        ")\n",
        "\n",
        "# SegFormer run\n",
        "wandb_logger_segformer = WandbLogger(\n",
        "    project=\"MMFlood_DL_Experiments\",\n",
        "    name=\"segformer\"\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(logger=wandb_logger, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "YTyVBwYwGC6u"
      },
      "id": "YTyVBwYwGC6u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "MMFlood dataset is a multimodal flood delineation dataset (Montello et al., 2022).\n",
        "\n",
        " Some Sentinel-1 tiles have missing data, which are automatically set to 0. Corresponding pixels in masks are set to 255 and should be ignored in performance computation.\n",
        "\n",
        "Dataset features:\n",
        "\n",
        "- 1,748 Sentinel-1 tiles of varying pixel dimensions\n",
        "\n",
        "- Multimodal dataset (Sentinel-1, DEMs and hydrography maps (available for 1,012 tiles out of 1,748))\n",
        "\n",
        "- 95 flood events from 42 different countries\n",
        "\n",
        "- Flood delineation maps (ground truth) is obtained from Copernicus EMS\n",
        "\n",
        "- Missing data in Sentinel-1 tiles are set to 0 and corrsponding pixels in masks are set to 255 (must ignored in performance computation)  \n",
        "\n",
        "Dataset classes:\n",
        "\n",
        "- no flood\n",
        "\n",
        "- flood\n",
        "\n"
      ],
      "metadata": {
        "id": "Y8O_i9lAGa-4"
      },
      "id": "Y8O_i9lAGa-4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deeplearning Models\n",
        "\n",
        "### a) Unet\n",
        "\n",
        "### b) SegFormer"
      ],
      "metadata": {
        "id": "X6XdBGY7IjDD"
      },
      "id": "X6XdBGY7IjDD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "fTlx-kD4LkLv"
      },
      "id": "fTlx-kD4LkLv"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_model(model_name, input_type, train_dataset, val_dataset, max_epochs=50, batch_size=8):\n",
        "    \"\"\"\n",
        "    Train a TorchGeo model (UNet, SegFormer) with dynamic wandb logging and checkpointing.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): 'unet' or 'segformer'\n",
        "        input_type (str): description of input bands, e.g., 's1_dem_hydro'\n",
        "        train_dataset, val_dataset: PyTorch Datasets\n",
        "        max_epochs (int): number of training epochs\n",
        "        batch_size (int): batch size\n",
        "    \"\"\"\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Define dynamic wandb logger\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    wandb_logger = WandbLogger(\n",
        "        project=\"MMFlood_DL_Experiments\",\n",
        "        name=f\"{model_name}_{input_type}_{timestamp}\"\n",
        "    )\n",
        "\n",
        "    # Define checkpoint callback\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor=\"val_iou\",\n",
        "        mode=\"max\",\n",
        "        save_top_k=1,\n",
        "        filename=f\"best-{model_name}-{{epoch:02d}}-{{val_iou:.4f}}\",\n",
        "        dirpath=f\"checkpoints/{model_name}_{input_type}\"\n",
        "    )\n",
        "\n",
        "    # Define TorchGeo Trainer (LightningModule)\n",
        "    task = SemanticSegmentationTask(\n",
        "        model=model_name,\n",
        "        in_channels=3,       # adjust based on your input\n",
        "        num_classes=2,\n",
        "        loss=\"ce\",\n",
        "        ignore_index=255\n",
        "    )\n",
        "\n",
        "    # Initialize PyTorch Lightning Trainer\n",
        "    trainer = Trainer(\n",
        "        max_epochs=max_epochs,\n",
        "        accelerator=\"auto\",\n",
        "        devices=1,\n",
        "        logger=wandb_logger,\n",
        "        callbacks=[checkpoint_callback]\n",
        "    )\n",
        "\n",
        "    # Fit model\n",
        "    trainer.fit(task, train_loader, val_loader)\n",
        "\n",
        "    # Return path of best checkpoint\n",
        "    return checkpoint_callback.best_model_path\n"
      ],
      "metadata": {
        "id": "xAswOQrIXB8g"
      },
      "id": "xAswOQrIXB8g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: train UNet\n",
        "best_unet_ckpt = train_model(\n",
        "    model_name=\"unet\",\n",
        "    input_type=\"s1_dem_hydro\",\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    max_epochs=50\n",
        ")\n",
        "\n",
        "# Example: train SegFormer\n",
        "best_segformer_ckpt = train_model(\n",
        "    model_name=\"segformer\",\n",
        "    input_type=\"s1_dem_hydro\",\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    max_epochs=50\n",
        ")\n",
        "\n",
        "print(\"Best UNet checkpoint:\", best_unet_ckpt)\n",
        "print(\"Best SegFormer checkpoint:\", best_segformer_ckpt)\n"
      ],
      "metadata": {
        "id": "R9OMhrW5LjCw"
      },
      "id": "R9OMhrW5LjCw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "Montello, F., Arnaudo, E., & Rossi, C. (2022). MMFlood: A Multimodal Dataset for Flood Delineation From Satellite Imagery. IEEE Access, 10, 96774â€“96787. https://doi.org/10.1109/ACCESS.2022.3205419\n",
        "\n"
      ],
      "metadata": {
        "id": "bh5MDimPH4qU"
      },
      "id": "bh5MDimPH4qU"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}